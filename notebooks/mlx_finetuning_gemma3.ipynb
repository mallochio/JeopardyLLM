{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning Gemma-3 Model on Jeopardy Data with MLX\n",
    "\n",
    "This notebook demonstrates a LoRA fine-tuning pipeline for the `gemma-3` model\n",
    "using the `mlx_lm` library. Our custom training data is located in\n",
    "`test_data/extra_matches.tsv` and contains Jeopardy questions & answers.\n",
    "\n",
    "Columns in the TSV:\n",
    "[ round, clue_value, daily_double_value, category, comments, answer, question, air_date, notes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uqq mlx mlx_lm transformers datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data\n",
    "data_path = Path(\"/Users/sid/Projects/code/JeopardyLLM/data\")\n",
    "df = pd.read_csv(f\"{data_path}/extra_matches.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 2. Format the data for Gemma's chat template\n",
    "def generate_prompt(row: pd.Series) -> str:\n",
    "    return f\"\"\"<bos><start_of_turn>user\n",
    "# Instructions\n",
    "# You are Alex Trebek hosting the current season of Jeopardy! You will provide a clue (the 'answer' in Jeopardy terms), and a contestant will respond with the correct 'question'. \n",
    "\n",
    "Jeopardy Round : {row['round']}\n",
    "Category : {row['category']}\n",
    "Value : {row['clue_value']}\n",
    "Air Date : {row['air_date']}\n",
    "Comments : {row['comments']}\n",
    "Notes : {row['notes']}\n",
    "Question : {row['question']}\n",
    "\n",
    "<end_of_turn> <start_of_turn>model Here is your Jeopardy clue: \"{row['answer']}\" <end_of_turn><eos>\"\"\"\n",
    "\n",
    "df[\"text\"] = df.apply(generate_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split into train/valid sets & save to JSONL\n",
    "split_index = int(len(df) * 0.9)\n",
    "df_shuf = df.sample(frac=1, random_state=42)\n",
    "train, valid = df_shuf[:split_index], df_shuf[split_index:]\n",
    "\n",
    "Path(f\"{data_path}/training_data\").mkdir(exist_ok=True)\n",
    "train[[\"text\"]].to_json(f\"{data_path}/training_data/train.jsonl\", orient=\"records\", lines=True)\n",
    "valid[[\"text\"]].to_json(f\"{data_path}/training_data/valid.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 8 files: 100%|████████████████████████| 8/8 [00:00<00:00, 23865.17it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.035% (0.459M/1301.876M)\n",
      "Starting training..., iters: 600\n",
      "Iter 1: Val loss 5.892, Val took 12.856s\n",
      "mx.metal.get_peak_memory is deprecated and will be removed in a future version. Use mx.get_peak_memory instead.\n",
      "Iter 10: Train loss 5.390, Learning Rate 1.000e-05, It/sec 0.904, Tokens/sec 548.091, Trained Tokens 6066, Peak mem 7.353 GB\n",
      "Iter 20: Train loss 4.186, Learning Rate 1.000e-05, It/sec 1.119, Tokens/sec 686.702, Trained Tokens 12201, Peak mem 7.353 GB\n",
      "Iter 30: Train loss 3.540, Learning Rate 1.000e-05, It/sec 1.125, Tokens/sec 690.792, Trained Tokens 18344, Peak mem 7.353 GB\n",
      "Iter 40: Train loss 3.086, Learning Rate 1.000e-05, It/sec 1.089, Tokens/sec 686.406, Trained Tokens 24648, Peak mem 7.353 GB\n",
      "Iter 50: Train loss 2.792, Learning Rate 1.000e-05, It/sec 1.018, Tokens/sec 635.116, Trained Tokens 30888, Peak mem 7.353 GB\n",
      "Iter 60: Train loss 2.494, Learning Rate 1.000e-05, It/sec 1.124, Tokens/sec 672.559, Trained Tokens 36872, Peak mem 7.353 GB\n",
      "Iter 70: Train loss 2.321, Learning Rate 1.000e-05, It/sec 1.053, Tokens/sec 656.109, Trained Tokens 43103, Peak mem 7.353 GB\n",
      "Iter 80: Train loss 2.071, Learning Rate 1.000e-05, It/sec 1.091, Tokens/sec 667.143, Trained Tokens 49219, Peak mem 7.353 GB\n",
      "Iter 90: Train loss 1.914, Learning Rate 1.000e-05, It/sec 0.963, Tokens/sec 598.076, Trained Tokens 55431, Peak mem 7.353 GB\n",
      "Iter 100: Train loss 1.692, Learning Rate 1.000e-05, It/sec 1.096, Tokens/sec 666.907, Trained Tokens 61515, Peak mem 7.353 GB\n",
      "Iter 100: Saved adapter weights to /Users/sid/Projects/code/JeopardyLLM/models/adapters.safetensors and /Users/sid/Projects/code/JeopardyLLM/models/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 1.587, Learning Rate 1.000e-05, It/sec 0.897, Tokens/sec 549.024, Trained Tokens 67635, Peak mem 7.353 GB\n",
      "Iter 120: Train loss 1.584, Learning Rate 1.000e-05, It/sec 1.146, Tokens/sec 711.235, Trained Tokens 73843, Peak mem 7.355 GB\n",
      "Iter 130: Train loss 1.631, Learning Rate 1.000e-05, It/sec 1.034, Tokens/sec 668.631, Trained Tokens 80308, Peak mem 8.704 GB\n",
      "Iter 140: Train loss 1.503, Learning Rate 1.000e-05, It/sec 0.906, Tokens/sec 570.264, Trained Tokens 86604, Peak mem 8.704 GB\n",
      "Iter 150: Train loss 1.414, Learning Rate 1.000e-05, It/sec 0.882, Tokens/sec 527.536, Trained Tokens 92588, Peak mem 8.704 GB\n",
      "Iter 160: Train loss 1.408, Learning Rate 1.000e-05, It/sec 0.926, Tokens/sec 559.447, Trained Tokens 98627, Peak mem 8.704 GB\n",
      "Iter 170: Train loss 1.314, Learning Rate 1.000e-05, It/sec 0.887, Tokens/sec 523.168, Trained Tokens 104526, Peak mem 8.704 GB\n",
      "Iter 180: Train loss 1.421, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 351.597, Trained Tokens 110762, Peak mem 8.704 GB\n",
      "Iter 190: Train loss 1.342, Learning Rate 1.000e-05, It/sec 1.029, Tokens/sec 628.605, Trained Tokens 116870, Peak mem 8.704 GB\n",
      "Iter 200: Val loss 1.320, Val took 10.750s\n",
      "Iter 200: Train loss 1.284, Learning Rate 1.000e-05, It/sec 1.140, Tokens/sec 676.361, Trained Tokens 122802, Peak mem 8.704 GB\n",
      "Iter 200: Saved adapter weights to /Users/sid/Projects/code/JeopardyLLM/models/adapters.safetensors and /Users/sid/Projects/code/JeopardyLLM/models/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 1.315, Learning Rate 1.000e-05, It/sec 1.117, Tokens/sec 672.600, Trained Tokens 128826, Peak mem 8.704 GB\n",
      "Iter 220: Train loss 1.304, Learning Rate 1.000e-05, It/sec 1.077, Tokens/sec 670.252, Trained Tokens 135050, Peak mem 8.704 GB\n"
     ]
    }
   ],
   "source": [
    "# 4. LoRA Fine-tuning on gemma-3 using mlx_lm\n",
    "# NOTE: Adjust --iters, --model, and hyperparameters as needed\n",
    "!python -m mlx_lm.lora \\\n",
    "    --model google/gemma-3-1b-it \\\n",
    "    --train \\\n",
    "    --iters 600 \\\n",
    "    --adapter-path /Users/sid/Projects/code/JeopardyLLM/models \\\n",
    "    --data /Users/sid/Projects/code/JeopardyLLM/data/training_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Merge LoRA weights to produce a fused model\n",
    "\"\"\"\n",
    "!python -m mlx_lm.fuse \\\n",
    "    --model google/gemma-3-1b-it \\\n",
    "    --adapter-file jeopardy_adapters.npz \\\n",
    "    --out-dir gemma3_jeopardy_fused\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (Optional) Test generation\n",
    "from mlx_lm import generate, load\n",
    "\n",
    "model, tokenizer = load(\"google/gemma-3-1b-it\", adapter_path=\"/Users/sid/Projects/code/JeopardyLLM/models/\")\n",
    "prompt = \"Hey! Are you Alex Trebek? Test me on Jeopardy!\"\n",
    "resp = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_tokens=6000,\n",
    "    verbose=True\n",
    ")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_jeopardy(model, tokenizer, n_samples=10):\n",
    "    \"\"\"\n",
    "    Generate a few sample Jeopardy clues and see how consistent they are.\n",
    "    Optionally, retrieve relevant references to reduce hallucinations.\n",
    "    \"\"\"\n",
    "    sample_indices = np.random.choice(len(val_df), size=n_samples, replace=False)\n",
    "    for idx in sample_indices:\n",
    "        row = val_df.iloc[idx]\n",
    "        # Retrieve relevant references for the clue from the vector store\n",
    "        # (Just a demonstration).\n",
    "        references = retrieve_similar_clues(row[\"answer\"], k=2)\n",
    "        \n",
    "        # Prepare a prompt.\n",
    "        prompt = f\"\"\"You are Alex Trebek:\n",
    "Clue: {row['answer']}\n",
    "(References: {references['answer'].tolist()})\n",
    "\n",
    "What is the best way to deliver this clue to the contestant?\n",
    "\"\"\"\n",
    "        # Hypothetical generation method\n",
    "        # response = model.generate(prompt, tokenizer, max_tokens=80)\n",
    "        # Evaluate correctness, style, or factual consistency as needed.\n",
    "        # ...\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
