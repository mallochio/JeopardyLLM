In this repo I aim to engineer an AI-driven Jeopardy! trainer using Retrieval-Augmented Generation (RAG) with a LoRA fine-tuned gemma 3:1B model. I want to significantly improve factual accuracy and reduce hallucinations via prompt engineering and custom evaluation metrics. In the #folder:data there are multiple tsv files containing jeopardy questions with the columns being "round clue_value daily_double_value category comments answer question air_date notes". Ollama is installed on my system which has the gemma3:1b already on disk. I am running this on a mac M1, so provide code so I can do the LoRA fine tuning on that. Use #websearch to understand more. Additionally, I will implement a logging mechanism to track training progress and evaluation metrics for better insights.